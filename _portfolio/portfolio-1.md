---
title: "Enabling Mobile Robots to Know Where They are at Given Time"
excerpt: "Visual odometry using pseudo map points, robust matching for more accurate feature correspondences in visual SLAM, lateral localization for reliable, urban autonomous driving."
collection: portfolio
---

**Visual Odometry Using Pseudo Map Points** A local, bundle-adjustment is an important procedure to improve the accuracy of a visual odometry solution. However, it is computationally very expensive as it jointly optimize all the poses of cameras and locations of map points. To reduce the computational complexity of such a procedure without much sacrificing the accuracy of a solution, the state- of-the-art algorithms [1],[2],[3] were proposed to eliminate the map point variables, using extra matrix operations, from their linearized optimization solutions. Instead of eliminating the map points, this paper proposes a novel way of addressing this complexity issue – we represent a map point as a function of two camera poses, and uses the triangulated location of the map point when needed. Our method is more efficient than a full-SLAM formulation in solving the visual odometry problem in that 1) the complexity of our solution is lower than those of the state-of-the-art methods, 2) no extra matrix operations required to eliminate map points, 3) no need guesses on map points’ initial locations. Experiemental results, through simulated experiments and experiments with KITTI data, demonstrated that our results are more accurate that those of a full-SLAM approach with lower runtime complexities.
Read the following paper to learn more about this work:
* Chih-Chung Chou, Chun-Kai Chang and **YoungWoo Seo**, [A structureless approach for visual odometry](https://ieeexplore.ieee.org/document/8500617), In *Proceedings of the IEEE Intelligent Vehicles Symposium* (IV-2018), pp. 1834-1841, 2018.

<br>

<img src="/images/two-stage-sampling.png"/>

**Robust Matching for More Accurate Feature Correspondences** For any visual feature based SLAM solutions, to estimate the relative camera motion between two images, it is necessary to find “correct” correspondence between features extracted from those images. Given a set of feature correspondents, one can use a n-point algorithm with robust estimation method, to produce the best estimate to the relative camera pose. The accuracy of a motion estimate is heavily dependent upon the accuracy of the feature correspondence. Such a dependency is even more significant when features are extracted from the images of the scenes with drastic changes in viewpoints and illuminations, and presence of occlusions. To make a feature matching robust to such challenging scenes, we propose a new feature matching method that incrementally chooses a five pairs of matched features for a full DoF camera motion estimation. In particular, at the first stage, we use our 2-point algorithm to estimate a camera motion, and at the second stage, use this estimated motion to choose three more matched features. In addition, we use, instead of the epipolar constraint, a planar constraint for more accurate outlier rejection. With this set of five matching fea- tures, we estimate a full DoF (Degree of Freedom) camera motion with scale ambiguity. Through the experiments with three, real-world datasets, our method demonstrates its effectiveness and robustness by successfully matching features 1) from the images of a night market where presenceof frequent occlusions and varying illuminations, 2) from the images of a night market taken by a handheld camera and by the Google street view, and 3) from the images of a same location taken daytime and nighttime. Read the following paper to learn more about this work:
* Chih-Chung Chou, **YoungWoo Seo**, and Chieh-Chih Wang, [A two-stage sampling for robust feature matching](https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21778), *Journal of Field Robotics*, 35(5): 779-801, 2018.  

<br>

<img src="/images/estimated-local-road-geometry.jpg"/>

**Lateral Localization for Reliable Urban Autonomous Driving** For safe urban driving, keeping a car within a road-lane boundary is a critical prerequisite. It requires human and robotic drivers to recognize the boundary of a road-lane and the vehicle's location with respect to the boundary of a road-lane that the vehicle happens to be driving on. To provide such a perception capability, we develop a new computer vision system that analyzes a stream of perspective images to produce information about a vehicle's location relative to the a road-lane's boundary, and information about the detecting of lane-crossing and lane-changing maneuvers. To assist the vehicle's lateral localization, our algorithm also estimates the host road-lane's geometry, including the number of road-lanes, their widths, and the index of the host road-lane. The local road geometry estimated by frame-by-frame may be inconsistent over frames, due to variations in the image features. To handle such inconsistent estimations, we implement an Unscented Kalman Filter (UKF) to smooth out, over time, the estimated road geometry. Tests on inter-city highway showed that our system provides stable and reliable performance in terms of computing lateral distances and detecting lane-crossing and lane-changing maneuvers. 
Read the following papers to learn more about this work:
* **Young-Woo Seo** and Myung Hwangbo, [A computer vision system for lateral localization](https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21576), *Journal of Field Robotics*, 32(7): 1004-1014, 2015.
* **Young-Woo Seo** and Raj Rajkumar, [Tracking and estimation of ego-vehicle's state for lateral localization](https://ieeexplore.ieee.org/document/6957859/), In *Proceedings of the 17th International IEEE Conference on Intelligent Transportation Systems* (ITSC-2014), pp. 1251-1257, Qingdao, China, Oct 8-11, 2014.
* **Young-Woo Seo** and Raj Rajkumar, [Use of a monocular camera to analyze a ground vehicle's lateral movements for reliable autonomous city driving](http://www.cs.cmu.edu/~youngwoo/doc/ppniv-13-ywseo.pdf), In *Proceedings of the 5th IEEE IROS Workshop on Planning, Perception and Navigation for Intelligent Vehicles* (PPNIV-2013), pp. 197-203, Nov 3-8, Tokyo, Japan, 2013.

<iframe width="560" height="315" src="https://www.youtube.com/embed/A09se5Z-s9A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
