<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Semantic Segmentation</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Semantic Segmentation</h1>
	
        <p>Semantic segmentation is a field of computer vision, where
        its goal is to assign each pixel of a given image to one of
        the predefined class labels, e.g., road, pedestrian, vehicle,
        etc. By estimating the class of each pixel, one could
        understand the scene depicted on a given image. This page
        describes an application of a fully convolutional network
        (FCN) for semantic segmentation.</p>
	
        <p class="view"><a href="http://github.com/orderedlist/minimal">View
        the Project on  GitHub</a></p>
	
      </header>
      
      <section>
        <h1>Training a Fully Convolutional Network (FCN) for Semantic Segmentation</h1>

	<h2>1. Introduction</h2>
        <p>Semantic segmentation is a computer vision task of
        assigning each pixel of a given image to one of the predefined
        class labels, e.g., road, pedestrian, vehicle, etc. By doing
        so, one can delineate, if it is successfully done, contours of
        all the objects appearing on the input image. For object
        detection/recognition, such an approach has a significant
        benefit over the conventional bounding box based
        classification: An exhaustive labeling of all the pixels on a
        given image, other than just putting rectangular boxes on a
        couple of objects. This page will describe the fully
        convolutional network and a way of implementing it using the
        TensorFlow.</p>

        <h2>2. Fully Convolutional Network (FCN) for Semantic Segmentation</h2>
	<h3>2.1 Overview</h3>
	<p>This section describes how to train a fully convolutional
	network for a pixelwise, binary classification: road or
	  not-road.</p>

	<p>A convolutional neural network is typically comprised of a
	series of convolution layers, followed by fully connected
	layers and a soft max activation function. Such architectures
	work really well for classification -- remember how do all
	these frenzies about deep neural network begin?  Yes,
	at <a href="http://webcache.googleusercontent.com/search?q=cache:http://image-net.org/challenges/LSVRC/2012/ilsvrc2012.pdf">the
	Large Scale Visual Recognition Challenge 2012</a> -- ImageNet
	classification,
	the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a>
	improved the classification accuracy drastically by dropping
	the error from 26% to 15%!</p>

	<img src="/semantic-segmentation/fig/yolo-output.png"/> <a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088">Source</a>
	
	<p>A typical output of a classification including that of the
	AlexNet is a bounding box around an object of interest. See
	the bounding boxes around objects like cars, bicyclist,
	traffic light from the above image. These bounding boxes tells
	us/robots where objects in the scene are so that our robots
	could navigate the scene without bumping into them. Of course,
	since a 2d image does not provide information about distances
	to the detected objects, the 3d information with respect to
	the detected objects is assumed to be, somehow, resolved. One
	could use a stereo camera or fuse images with lidar
	measurements to estimate distances to the objects. There have
	been so many great classification
	algorithms: <a href="https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">SVMs
	with HoG</a> as a canonical example of non-DNN
	based, <a href="https://pjreddie.com/darknet/yolo/">Yolo</a>, <a href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06">SSD</a>
	as DNN/CNN based classification/object detection algorithms,
	etc.</p>

	<p>Read the following to learn more about these DNN-based,
	  object detection algorithms:
	  <ul>
	    <li><a href="https://mc.ai/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo/">Object detection: speed and accuracy comparison (Faster RCNN, R-FCN, SSD and Yolo)</a></li>
	  </ul>
	</p>
	
<img src="/semantic-segmentation/fig/ssd-output.png"/> <a href="https://arxiv.org/abs/1512.02325">Source</a>
	
	<p>But what if we want to know the pixel positions of road's
	boundary, instead of the bounding boxes of cars on that road?
	Can our awesome CNN (Convolutional Neural Network) do such a
	task for us? Unfortunately, no, they cannot, because the fully
	connected layers at the end of any CNNs do not preserve any
	spatial information. Fortunately, the fully convolutional
	network (FCN) can do it for us. This is because, while doing
	the convolution, they preserve the spatial information
	throughout the entire network. In addition, since
	convolutional operations fundamentally don't care about the
	size of the input, a FCN will work on images of any size. In a
	classic convolutional network with fully connected final
	layers, the size of the input is constrained by the size of
	the fully connected layers. Passing different size images
	through the same sequence of convolutional layers and
	flattening the final output. These outputs would be of
	different sizes, which doesn't work very well for matrix
	  multiplication.</p>

	<p>Read the followings to learn more about
	FCN-based semantic segmentation.

	<ul>
	  <li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review">A
	  2017 guide to semantic segmentation with deep
	      learning</a></li>
	  <li><a href="http://deeplearning.net/tutorial/fcn_2D_segm.html">Fully convolution networks for 2d segmentation</a></li>
	</ul>
	</p>
	
	<p>How does a FCN then accomplish such a task? It can do such
	a task for us based, primarily, on three special techniques at
	the top of a CNN:
	  <ul>
	    <li>1x1 convolutioinal layers,</li>
	    <li>up-sampling, and</li>
	    <li>skip connections.</li>
	  </ul></p>

	<p>A FCN is typically comprised of two parts: encoder and
	  decoder. A encoder extracts features that will later be used
	  by the decoder. It's common to use the encoder pre-trained
	  on ImageNet. As examples, VGG and ResNet are popular
	  choices. Since this is conceptually the same as a transfer
	  learning, we can use techniques from transfer learning to
	  accelerate the training of our FCNs. To convert a CNN into a
	  FCN, the first thing we need to do is to convert the fully
	  connected layer into the 1x1 convolutional layer. The
	  encoder is followed by the decoder, which uses another
	  technique of transposed convolutional layers to upsample the
	  image. Lastly the skip connection is added. While utilizing
	  the skip connections, one'll have to be careful not to add
	  too many skip connections. Otherwise it can lead to the
	  explosion in the size of your model. For example, when using
	  VGG-16 as the encoder, only the third and the fourth pooling
	  layers are typically used for skip connections. Once we're
	  done with incorporating all these three techniques, we can
	  train the model end to end as we do for a CNN. Read the
	  followings to learn more about encoder and decoder
	  architecture.
	  <ul>
	    <li><a href="https://arxiv.org/pdf/1511.00561.pdf">SegNet:
	A deep convolutional encoder-decoder architecture for image
		segmentation</a></li>
	    <li><a href="https://courses.cs.washington.edu/courses/cse576/17sp/notes/Sachin_Talk.pdf">Encoder-decoder networks for semantic segmentation</a></li>
	  </ul>

	  Now let's take a look at each of the three techniques in
	  details:
	</p>

	<h3>2.2 1x1 Convolution Layer</h3>

	<p>What do we mean by using 1x1 convolutional layers? It is to
	replace a fully connected layer of a CNN with 1x1
	convolutional layers. By doing so, the output of the
	convolution operation is the result of sweeping the kernel
	over the input with the sliding window and performing element
	wise multiplication and summation. One way to think about this
	is the number of kernels is equivalent to the number of
	outputs in a fully connected layer. Similarly, the number of
	weights in each kernel is equivalent to the number of inputs
	in the fully connected layer. Effectively, this turns
	convolutions into a matrix multiplication with spatial
	information.</p>

	<p>In TensorFlow, one can implement 1x1 convolutional layer as: </p>
<pre class="brush: python">
  tf.layers.conv2d(x, num_outputs, kernel_size=1, stride=1, weights_initializer=custom_init)
</pre>
	<p>where,
	  <ul>
	    <li>num_outputs: the number of output channels or kernels</li>
	    <li><a href="http://cs231n.github.io/neural-networks-2/#init">weight initialization</a></li>
	    <li><a href="http://cs231n.github.io/neural-networks-2/#reg">regularization</a></li>
	  </ul>
	</p>
	  
	<h3>2.3 Up-sampling </h3>

	<p>An up-sampling is done through a transposed convolution. A
	transposed convolution is essentially a reverse convolution
	where the forward and the backward passes are swapped -- this
	is why it is called transpose convolution. Some people call it
	deconvolution because it literally undoes the previous
	convolution. Note that the math is exactly the same because
	what we're doing is swapping the order of forward and backward
	  passes.</p>

	<p>Such transposed convolutions help in upsampling the
	  previous layer to a higher resolution or dimension. Let's
	  take an example: Suppose you have a 3x3 input and you wish
	  to upsample that to the desired dimension of 6x6. The
	  process involves multiplying each pixel of your input with a
	  kernel or filter. If this filter was of size 5x5, the output
	  of this operation will be a weighted kernel of size
	  5x5. This weighted kernel then defines your output
	  layer. However, the upsampling part of the process is
	  defined by the strides and the padding. In TensorFlow, using
	  the `tf.layers.conv2d_transpose`, a `stride` of 2, and
	  `"SAME"` padding would result in an output of dimensions
	  6x6. See the followings to learn more about up-sampling.</p>
	<ul>
	  <li><a href="https://github.com/vdumoulin/conv_arithmetic">Animations about convolution arithmetic</a></li>	  
	  <li><a href="https://arxiv.org/abs/1603.07285">A guide to convolution arithmetic for deep learning</a></li>
	</ul>

	<p>In TensorFlow, </p>
<pre class="brush: python">
  tf.layers.conv2d_transpose(x, 3, (2, 2), (2,2))
</pre>

	<h3>2.4 Skip Connections</h3>
	<p>The third special technique that a FCN uses is the skip
	connection. One effect of convolutions or encoding in general
	is you narrow down the scope by looking closely at some
	picture and lose the bigger picture as a result. So even if we
	were to decode the output of the encoder back to the original
	image size, some information has been lost. Skip connections
	are a way of retaining the information easily. The way skip
	connection work is by connecting the output of one layer to a
	non-adjacent layer. </p>

	<h3>2.5 Implementation of a FCN</h3>
	<p>This section describes how to train a fully convolutional
	network for a pixelwise, binary classification: road or
	  not-road.</p>

	<img src="/semantic-segmentation/fig/cityscape-example-zurich.png" />

	<p>The above image is an example of
	the <a href="https://www.cityscapes-dataset.com/">CityScapes</a>
	dataset where each pixl in the image is painted with different
	color based on what the target class it's belonging to. The
	conceivable target classes include road, car, pedestrian,
	sign, monorail, etc. Thus by doing semantic segmentation one
	will get valuable information about the scene appearing on the
	image, instead of slicing sections into bounding
	boxes. Semantic segmentation is also known as scene
	understanding, particularly for the field of autonomous
	  driving. Read the followings to learn more about this topic.
	  <ul>
	    <li><a href="https://arxiv.org/abs/1708.02550">Fast scene understanding for autonomous driving</a></li>
	    <li><a href="http://cvrsuad.droppages.com/">ECCV-18 Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving</a></li>
	    <li><a href="https://link.springer.com/article/10.1007/s11633-018-1126-y">A survey of scene understanding by event reasoning in autonomous driving</a></li>
	  </ul>
	</p>

	<p>The FCN we are primarily based on is
	  the <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">FCN-8</a>
	  The encoder for FCN-8 is the VGG16 model pretrained on
	  ImageNet for classification. The fully-connected layers are
	  replaced by 1-by-1 convolutions. Here’s an example of going
	  from a fully-connected layer to a 1x1 convolution in
	  TensorFlow:
	  
<pre class="brush: python">
  num_classes = 2
  output = tf.layers.dense(input, num_classes)
</pre>
To
<pre class="brush: python">
  num_classes = 2
  output = tf.layers.conv2d(input, num_classes, 1, strides=(1,1))
</pre>
	</p>

	<p>To build the decoder portion of FCN-8, next we’ll have to
	upsample the input to the original image size. The shape of
	the tensor after the final convolutional transpose layer will
	be 4-dimensional: (batch_size, original_height,
	original_width, num_classes). Let’s implement those transposed
	  convolutions we discussed earlier as follows:
<pre class="brush: python">
  output = tf.layers.conv2d_transpose(input, num_classes, 4, strides=(2,2))
</pre>
The transpose convolutional layers increase the height and width
dimensions of the 4D input Tensor.</p>

	<p>The final step is adding skip connections to the model. In
	order to do this we’ll have to combine the output of two
	layers. The first output is the output of the current
	layer. The second output is the output of a layer further back
	in the network, typically a pooling layer. In the following
	example we combine the result of the previous layer with the
	result of the 4th pooling layer through elementwise addition
	  (`tf.add`).
<pre class="brush: python">
input = tf.add(input, pool_4)
</pre>
	  We can then follow this with another transposed convolution layer.
<pre class="brush: python">
input = tf.layers.conv2d_transpose(input, num_classes, 4, strides=(2, 2))
</pre>

	  We’ll repeat this once more with the third pooling layer output.
<pre class="brush: python">
input = tf.add(input, pool_3)
Input = tf.layers.conv2d_transpose(input, num_classes, 16, strides=(8, 8))
</pre>

	<p>Now we're done in converting a CNN, vgg16, into a FCN:
	converting a fully connected layer into an 1x1 convolutional
	layer, doing up-sampling, and skipping connections. The last
	step one needs to do is to define the details of training:
	defining the loss function. A good news is that there is
	nothinig special we need to do -- just training exactly same
	as the way we train a normal classification CNN, even with the
	goal of a FCN is different from that of CNN. Yes, we can still
	use the cross entropy loss. But one thing we have to remember
	  the output tensor is 4D so we have to reshape it to 2D:
	  
<pre class="brush: python">
  logits = tf.reshape(input, (-1, num_classes))
</pre>
where, `logits` is now a 2D tensor where each row represents a pixel
and each column a class. From here we can just use standard cross
entropy loss:
<pre class="brush: python">
  cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))
</pre>

That’s it, we cover everything we need to do semantic
segmentation. </p>

	<p>We need to load a pretrained vgg </p>
	
<pre class="brush: python">
def load_vgg(sess, vgg_path):
    """
    Load a pre-trained network, VGG for this case, into TensorFlow as an encoder.

    :param sess: TensorFlow Session
    :param vgg_path: Path to vgg folder, containing "variables/" and "saved_model.pb"

    :return: Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)
    """

    vgg_tag = 'vgg16'
    vgg_input_tensor_name = 'image_input:0'
    vgg_keep_prob_tensor_name = 'keep_prob:0'
    vgg_layer3_out_tensor_name = 'layer3_out:0'
    vgg_layer4_out_tensor_name = 'layer4_out:0'
    vgg_layer7_out_tensor_name = 'layer7_out:0'

    # Use tf.saved_model.loader.load to load the model and weights
    # tf.saved_model.loader.load() returns a protobuf
    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)

    # Get tensors to return
    image_input = sess.graph.get_tensor_by_name(vgg_input_tensor_name)
    keep_prob = sess.graph.get_tensor_by_name(vgg_keep_prob_tensor_name)
    layer3_out = sess.graph.get_tensor_by_name(vgg_layer3_out_tensor_name)
    layer4_out = sess.graph.get_tensor_by_name(vgg_layer4_out_tensor_name)
    layer7_out = sess.graph.get_tensor_by_name(vgg_layer7_out_tensor_name)

    return image_input, keep_prob, layer3_out, layer4_out, layer7_out's
</pre>

	<p>We're going to create a FCN by 1) converting a fully
	connected layer into 1x1 convolutional layer, 2) doing
	  up-sampling and 3) skipping connections.
	  
<pre class="brush: python">	  
def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):
    """
    Create a fully convolutional network, using the vgg layers, with 1x1
    convolution, skip-connection, transpose for upsampling

    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output
    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output
    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output
    :param num_classes: Number of the target classes

    :return: The Tensor for the last layer of output

    """

    # Given a pre-trained network (VGG) as encoder
    
    # A function, from the class, for creating 1x1 convolution layer
    def conv_1x1(x, num_outputs):
        kernel_size = 1
        stride = 1
        # regularization makes output much appealing!
        return tf.layers.conv2d(x, num_outputs, kernel_size, stride,
                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                                kernel_regularizer = tf.contrib.layers.l2_regularizer(1e-3)) 

    # num_outputs/kernels should be the same as
    # num_classes. num_classes should be 2 for this project -- a
    # binary classification: road or non-road?
    conv_out = conv_1x1(vgg_layer7_out, num_classes)

    # Transposed/backward convolutions for creating a decoder
    deconv_1 = tf.layers.conv2d_transpose(conv_out, num_classes, 4, 2, 'SAME',
                                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                                          kernel_regularizer = tf.contrib.layers.l2_regularizer(1e-3))
    
    # Add a skip connection to previous VGG layer
    skip_layer_1 = conv_1x1(vgg_layer4_out, num_classes)
    skip_conn_1 = tf.add(deconv_1, skip_layer_1)

    # Up-sampling
    deconv_2 = tf.layers.conv2d_transpose(skip_conn_1, num_classes, 4, 2, 'SAME',
                                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                                          kernel_regularizer = tf.contrib.layers.l2_regularizer(1e-3))

    # Add another skip connection to previous VGG layer
    skip_layer_2 = conv_1x1(vgg_layer3_out, num_classes)
    skip_conn_2 = tf.add(deconv_2, skip_layer_2)

    # Up-sampling again
    deconv_3 = tf.layers.conv2d_transpose(skip_conn_2, num_classes, 16, 8, 'SAME',
                                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                                          kernel_regularizer = tf.contrib.layers.l2_regularizer(1e-3))

    return deconv_3
</pre>

	<p>Let's define a setup for training a neural network.</p>
<pre class="brush: python">
def optimize(nn_last_layer, correct_label, learning_rate, num_classes):
    """
    Optimize the fcn by defining the loss function and the optimization method.

    :param nn_last_layer: TF Tensor of the last layer in the neural network
    :param correct_label: TF Placeholder for the correct label image
    :param learning_rate: TF Placeholder for the learning rate
    :param num_classes: Number of classes to classify

    :return: Tuple of (logits, train_op, cross_entropy_loss)
    """
    
    # Reshape predictions and labels through flattening
    logits = tf.reshape(nn_last_layer, (-1, num_classes))
    labels = tf.reshape(correct_label, (-1, num_classes))

    # Define the loss function
    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))

    # Define optimizer, ADAM
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)

    # Define train_op to minimise loss
    train_op = optimizer.minimize(cross_entropy_loss)

    return logits, train_op, cross_entropy_loss
</pre>

	<p>Let's define the procedure of training a neural network.</p>

<pre class="brush: python">	  	
def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,
             correct_label, keep_prob, learning_rate):
    """
    Train the fcn.

    :param sess: TF Session
    :param epochs: Number of epochs
    :param batch_size: Batch size
    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)
    :param train_op: TF Operation to train the neural network
    :param cross_entropy_loss: TF Tensor for the amount of loss
    :param input_image: TF Placeholder for input images
    :param correct_label: TF Placeholder for label images
    :param keep_prob: TF Placeholder for dropout keep probability
    :param learning_rate: TF Placeholder for learning rate
    """

    # define hyper-parameters
    keep_prob_stat = 0.8
    learning_rate_stat = 1e-4
    
    for epoch in range(epochs):
        for image, label in get_batches_fn(batch_size):
            _, loss = sess.run([train_op, cross_entropy_loss],
                               feed_dict={input_image: image,
                                          correct_label: label,
                                          keep_prob: keep_prob_stat,
                                          learning_rate:learning_rate_stat})
            
        print("Epoch %d of %d: Training loss: %.4f" %(epoch+1, epochs, loss))
</pre>

	<p>Let's finally put everything together.</p>
<pre class="brush: python">	
def run():
    num_classes = 2
    image_shape = (160, 576)
    data_dir = './data'
    runs_dir = './runs'
    
    tests.test_for_kitti_dataset(data_dir)

    # Set hyper-parameters
    epochs = 20
    # so far, the only option that works is the one w/ batch_size 1
    batch_size = 1
    
    # Download the pretrained vgg model
    helper.maybe_download_pretrained_vgg(data_dir)

    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.
    # You'll need a GPU with at least 10 teraFLOPS to train on.
    #  https://www.cityscapes-dataset.com/

    with tf.Session() as sess:
        # Path to vgg model
        vgg_path = os.path.join(data_dir, 'vgg')
        
        # Create function to get batches
        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)

        # OPTIONAL: Augment Images for better results
        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network
        
        # TODO: Build NN using load_vgg, layers, and optimize function        
        image_input, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)

        # Build a FCN
        last_layer = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)

        correct_label = tf.placeholder(dtype=tf.float32, shape=(None, None, None, num_classes))
        learning_rate = tf.placeholder(dtype=tf.float32)

        # Make optimization ready
        logits, train_op, cross_entropy_loss = optimize(last_layer, correct_label, learning_rate, num_classes)
        
        # Train the fcn using the train_nn function
        sess.run(tf.global_variables_initializer())
        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, image_input,
                 correct_label, keep_prob, learning_rate)

        # Save inference data using helper.save_inference_samples
        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, image_input)
</pre>
  
        <h2>3. Results</h2>

	<p>This section describes how to run the above explained
	code. The first order business is to make your machine
	ready by installing all the necessary packages.</p>
	
	<h3>3.1 Setup</h3>

	<p>In order to make the above described code run, you'll need
	  GPU. If the machine you're using doesn't have any fancy
	  Nvidia's <a href="https://www.nvidia.com/en-in/geforce/">GPU
	  boards</a>, you might want to consider to use GPU-enabled
	  cloud service like AWS or other equivalents. See the
	  following to learn how to set up AWS EC (Elastic Compute).
	  <ul>
	    <li><a href="https://aws.amazon.com/ec2/getting-started/">Getting started with Amazon EC2</a></li>
	    <li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html">Setting up with Amazon EC2</a></li>
	    <li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html">Getting started with Amazon EC2 linux instances</a></li>
	    <li><a href="https://medium.com/@dichen_5479/launch-an-aws-ec2-instance-with-gpu-for-deep-learning-in-10minutes-685e5c025ec6">Launch an AWS EC2 instance with GPU for Deep Learning</a></li>
	    <li><a href="https://www.datacamp.com/community/tutorials/aws-ec2-beginner-tutorial">AWS EC2 tutorial for beginners</a></li>
	  </ul>
	</p>

	<p>
	  The packages you need
	<ul>
	  <li><a href="https://www.python.org">Python 3</a></li>
	  <li><a href="https://www.tensorflow.org">TensorFlow</a></li>
	  <li><a href="https://www.numpy.org">Numpy</a></li>
	  <li><a href="https://www.scipy.org">SciPy</a></li>
	</ul>
	</p>
	
	<h3>3.2 Dataset</h3>
	<p>There is a couple of dataset for semantic segmentation.
	<ul>
	  <li><a href="http://www.cvlibs.net/datasets/kitti/eval_road.php">KITTI
	      Road Dataset</a></li>
	  <li><a href="https://www.cityscapes-dataset.com/examples/">CityScapes Dataset</a></li>
	  <li><a href="http://bdd-data.berkeley.edu/">Berkeley DeepDrive Dataset</a></li>
	</ul>
	For this, we used the KITTI road dataset.
	</p>

	<h3>3.3 Results</h3>
	<p>The animated image at the below shows some of the
	  segmentation results. For this project, we did not do a
	  full-scale, semantic segmentation. Instead we did a binary
	  classification.</p>

	<img src="/semantic-segmentation/fig/semantic-segmentation-output.gif"/>
	
	<h2>4. Summary</h2>
	<p>This pages describes what a fully convolutional network
	(FCN) and how it is used for semantic segmentation.</p>
	
      </section>
      <footer>
        <p>This page is scribbled by <a href="http://ywseo.github.io/">Youngwoo Seo</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
